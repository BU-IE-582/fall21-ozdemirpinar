{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "title: \"Homework_5\"\n",
    "author: \"Pınar Özdemir\"\n",
    "date: \"27/01/2022\"\n",
    "#---\n",
    "\n",
    "# Required libraries\n",
    "library(tidyr)\n",
    "library(openxlsx)\n",
    "library(xlsx)\n",
    "library(data.table)\n",
    "library(Matrix)\n",
    "library(tidyverse)\n",
    "library(dplyr)\n",
    "library(glmnet)\n",
    "library(caret)\n",
    "library(rpart)\n",
    "library(randomForest)\n",
    "library(e1071)\n",
    "library(ranger)\n",
    "library(gbm)\n",
    "library(pROC)\n",
    "library(skimr)\n",
    "library(corrplot)\n",
    "library(TunePareto)\n",
    "library(ggplot2)\n",
    "library(fastmatch)\n",
    "library(smotefamily)\n",
    "library(cluster)\n",
    "library(factoextra)\n",
    "\n",
    "\n",
    "# Reading and showing data\n",
    "train_dataset <- read.csv(\"C:/Users/Pinar/Downloads/train.csv\", header = T)\n",
    "#head\n",
    "head(train_dataset)\n",
    "#skim (1715 rows and 63 columns)\n",
    "skim(train_dataset)\n",
    "#glimpse\n",
    "glimpse(train_dataset)\n",
    "# dimension\n",
    "dim(train_dataset)\n",
    "\n",
    "# Checking for missing value \n",
    "train_dataset[train_dataset  == \"\"] <- NA\n",
    "train_dataset [train_dataset  == \" \"] <- NA\n",
    "\n",
    "# counting for NA\n",
    "listofna <- cbind(lapply(lapply(train_dataset, is.na), sum))\n",
    "listofna # there are any NA\n",
    "\n",
    "# looking the train data set\n",
    "str(train_dataset)\n",
    "\n",
    "# changing Var_39 and Var_53 features to numeric\n",
    "#for Var_39\n",
    "unique(train_dataset$Var_39)\n",
    "train_dataset$Var_39=ifelse(train_dataset$Var_39== \"Y\",1,0)\n",
    "#for Var_53\n",
    "unique(train_dataset$Var_53)\n",
    "train_dataset$Var_53=ifelse(train_dataset$Var_53== \"Y\",1,0)\n",
    "\n",
    "# Class distribution of train data set and correlation matrix \n",
    "table(train_dataset$default)\n",
    "ggplot(train_dataset, aes(x=default)) + geom_bar(color=\"blue\", fill = \"white\")\n",
    "# check correlation\n",
    "# creating correlation matrix without loan_application_id\n",
    "correlation_matrix = cor(train_dataset[,-1])\n",
    "corrplot(correlation_matrix, method = \"color\")\n",
    "\n",
    "# omitting loan_application_id from train data set\n",
    "train_dataset =  train_dataset [,-1]\n",
    "head(train_dataset)\n",
    "\n",
    "# Creating new train and test part from train_dataset with 0.7 and 0.3 ratios\n",
    "set.seed(1)\n",
    "index <- createDataPartition(train_dataset$default, p=0.7, list=FALSE)\n",
    "trainn <- train_dataset[ index,]\n",
    "testt <- train_dataset[-index,]\n",
    "sum(trainn$default)\n",
    "# 115 \n",
    "length(trainn$default)  \n",
    "# 1201\n",
    "length(testt$default) \n",
    "# 514\n",
    "sum(testt$default) \n",
    "# 57\n",
    "\n",
    "# Class distribution of new train data set  \n",
    "table(trainn$default)\n",
    "ggplot(trainn, aes(x=default)) + geom_bar(color=\"blue\", fill = \"purple\")\n",
    "# 1086, 115\n",
    "\n",
    "head(trainn)\n",
    "\n",
    "# scaling except default for train and test subset data\n",
    "# for train subset\n",
    "scaled_train <- as.data.table(scale(trainn[,-c(1,2)]))\n",
    "# addition of default\n",
    "scaled_trainn <- cbind(scaled_train, trainn$default)\n",
    "colnames(scaled_trainn)[61] <-  \"default\" \n",
    "\n",
    "# for test subset\n",
    "scaled_test <- as.data.table(scale(testt[,-c(1,2)]))\n",
    "# addition of default\n",
    "scaled_testt <- cbind(scaled_test, testt$default)\n",
    "colnames(scaled_testt)[61] <-  \"default\" \n",
    "\n",
    "### Calculation for a custom performance metric\n",
    "# for pred=1, obs=1 - 0\n",
    "# for pred=1, obs=0 - (0,15)x(loan_amount)\n",
    "# for pred=0, obs=1 - (loan_amount)\n",
    "# for pred=0, obs=0 - 0\n",
    "\n",
    "### Creating function for custom performance metric of total money lost for train data\n",
    "\n",
    "totalcost_func = function(data, lev = NULL , model= NULL ){\n",
    "data$pred <- as.numeric(data$pred)-1\n",
    "data$obs <- as.numeric(data$obs)-1 \n",
    "listofloan <- trainn$loan_amount[data$rowIndex]\n",
    "cost= 0\n",
    "for (a in 1:length(data$obs)){\n",
    "if  ( data$obs [a] == 1  & data$pred[a] == 0  ){\n",
    "cost <- cost+listofloan [a]}\n",
    "if  ( data$obs [a] == 0  & data$pred[a] == 1 ){\n",
    "cost = cost+listofloan [a] * (0.15)}}\n",
    "names(cost) <- c('total money lost')\n",
    "cost\n",
    "}\n",
    "\n",
    "### Creating the function to observe performance of test data\n",
    "cost_test <- function(pred, true ){\n",
    "te_co <- 0\n",
    "for (b in 1:length(pred)){\n",
    "if  (true [b] == 1 & pred[b] == 0){\n",
    "te_co <- te_co+testt$loan_amount [b]}\n",
    "if  (true [b] == 0 & pred[b] == 1){\n",
    "te_co <- te_co+testt$loan_amount [b] * (0.15)}} \n",
    "names(te_co) <- c('total money lost_test')\n",
    "te_co\n",
    "}\n",
    "\n",
    "# In modeling part Stochastic Gradient Boosting (SGB) approach is adopted for the classification goal.\n",
    "#model was trained with scaled_trainn data set and tested on scaled_testt\n",
    "\n",
    "# to avoid class imbalance problem, up, down and smote sampling techniques were used.\n",
    "\n",
    "# Model1: SGB - up sampling\n",
    "\n",
    "set.seed(1)\n",
    "train_cont_up <- trainControl(summaryFunction = totalcost_func, method=\"cv\", number=10, savePredictions=TRUE, sampling=\"up\")\n",
    "up_grid=expand.grid(interaction.depth = c(3,5,7), n.trees = c(300,500,700), shrinkage = c(0.01),n.minobsinnode = c(3,5,7))\n",
    "up_train <- train(as.factor(default) ~., data = scaled_trainn,\n",
    "                  method = \"gbm\",  metric = \"total money lost\" ,\n",
    "                  tuneGrid = up_grid ,\n",
    "                  trControl = train_cont_up)\n",
    "\n",
    "up_train\n",
    "up_train$finalModel\n",
    "\n",
    "pred_up <- predict(up_train, scaled_testt[,-61])\n",
    "table(pred_up,  as.factor(scaled_testt$default))\n",
    "\n",
    "con_mat_up <- confusionMatrix(pred_up , as.factor(testt$default))\n",
    "con_mat_up\n",
    "\n",
    "te_co_up <- cost_test(pred_up,  scaled_testt$default)\n",
    "te_co_up\n",
    "\n",
    "# Model2: SGB - down sampling\n",
    "\n",
    "set.seed(1)\n",
    "train_cont_down <- trainControl(summaryFunction = totalcost_func, method=\"cv\", number=10, savePredictions=TRUE, sampling=\"down\")\n",
    "down_grid <- expand.grid(interaction.depth = c(3,5,7), n.trees = c(300,500,700), shrinkage = c(0.01),n.minobsinnode = c(3,5,7))\n",
    "down_train <- train(as.factor(default) ~., data = scaled_trainn,\n",
    "                  method = \"gbm\",  metric = \"total money lost\" ,\n",
    "                  tuneGrid = down_grid ,\n",
    "                  trControl = train_cont_down) \n",
    "\n",
    "pred_down <- predict(down_train, scaled_testt[,-61])\n",
    "table(pred_down,  as.factor(scaled_testt$default))\n",
    "\n",
    "con_mat_down <- confusionMatrix(pred_down , as.factor(testt$default))\n",
    "con_mat_down\n",
    "\n",
    "te_co_down <- cost_test(pred_down,  scaled_testt$default)\n",
    "te_co_down\n",
    "\n",
    "# Model3: SGB - smote sampling\n",
    "\n",
    "set.seed(1)\n",
    "train_cont_smote <- trainControl(summaryFunction = totalcost_func, method=\"cv\", number=10, savePredictions=TRUE, sampling=\"smote\")\n",
    "smote_grid <- expand.grid(interaction.depth = c(3,5,7), n.trees = c(300,500,700), shrinkage = c(0.01),n.minobsinnode = c(3,5,7))\n",
    "smote_train <- train(as.factor(default) ~., data = scaled_trainn,\n",
    "                  method = \"gbm\",  metric = \"total money lost\" ,\n",
    "                  tuneGrid = smote_grid ,\n",
    "                  trControl = train_cont_smote) \n",
    "\n",
    "pred_smote <- predict(smote_train, scaled_testt[,-61])\n",
    "table(pred_smote,  as.factor(scaled_testt$default))\n",
    "\n",
    "con_mat_smote <- confusionMatrix(pred_smote , as.factor(testt$default))\n",
    "con_mat_smote\n",
    "\n",
    "te_co_smote <- cost_test(pred_smote,  scaled_testt$default)\n",
    "te_co_smote\n",
    "\n",
    "### Comparison for up, down and smote sampling method on SGB model\n",
    "\n",
    "model_comparison = resamples(list(SGB_up_sampling = up_train,\n",
    "                                  SGB_down_sampling = down_train,\n",
    "                                  SGB_smote_sampling = smote_train))\n",
    "\n",
    "# values and summary for models\n",
    "model_comparison$values\n",
    "summary(model_comparison)\n",
    "\n",
    "# representation of each model total money losts\n",
    "bwplot(model_comparison , horizontal = T)\n",
    "\n",
    "### after comparison Model1: SGB - up sampling was selected due to giving minimum total money lost\n",
    "# this result also shows suitability of up-sampling on such our data.\n",
    "# because up-sampling technique replicates the observations from minority class to balance the data.\n",
    "\n",
    "### Calling real test data and applying same steps as in train\n",
    "test_dataset <- read.csv(\"C:/Users/Pinar/Downloads/train.csv\", header = T)\n",
    "#head\n",
    "head(test_dataset)\n",
    "#skim \n",
    "skim(test_dataset)\n",
    "#glimpse\n",
    "glimpse(test_dataset)\n",
    "# dimension\n",
    "dim(test_dataset)\n",
    "# 1715 rows, 63 columns\n",
    "\n",
    "# changing Var_39 and Var_53 features to numeric\n",
    "#for Var_39\n",
    "unique(test_dataset$Var_39)\n",
    "test_dataset$Var_39=ifelse(test_dataset$Var_39== \"Y\",1,0)\n",
    "#for Var_53\n",
    "unique(test_dataset$Var_53)\n",
    "test_dataset$Var_53=ifelse(test_dataset$Var_53== \"Y\",1,0)\n",
    "\n",
    "# scaling except default for real test data\n",
    "scaled_testtt <- as.data.table(scale(test_dataset[,-c(1,3)]))\n",
    "# dimension\n",
    "dim(scaled_testtt)\n",
    "# 1715 rows, 61 columns\n",
    "\n",
    "# Utilization of selected model (Model1:SGB-up sampling) on whole train data set\n",
    "\n",
    "# scaling except default and loan columns\n",
    "sca_train <- as.data.table(scale(train_dataset[,-c(1,2)]))\n",
    "# addition of default\n",
    "w_scaled_train = cbind(sca_train, train_dataset$default)\n",
    "colnames(w_scaled_train)[60] <-  \"default\" \n",
    "\n",
    "# Model1: SGB - up sampling FOR WHOLE TRAIN DATA SET\n",
    "\n",
    "# the best tuning parameter for model1\n",
    "#n.trees= 700\n",
    "#interaction.depth = 5\n",
    "#shrinkage = 0.01\n",
    "#n.minobsinnode = 7\n",
    "\n",
    "set.seed(1)\n",
    "w_train_cont_up <- trainControl(summaryFunction = totalcost_func, method=\"cv\", number=10, savePredictions=TRUE, sampling=\"up\")\n",
    "w_up_train <- train(as.factor(default) ~., data = w_scaled_train,\n",
    "                  method = \"gbm\",  metric = \"total money lost\" ,\n",
    "                  tuneGrid = expand.grid(interaction.depth = c(5), n.trees = c(700), shrinkage = c(0.01),n.minobsinnode = c(7)),\n",
    "                  trControl = w_train_cont_up)\n",
    "\n",
    "# final model outputs\n",
    "w_up_train\n",
    "w_up_train$finalModel\n",
    "\n",
    "# predictions for final model\n",
    "w_pred_up <- predict(w_up_train, scaled_testtt)\n",
    "length(w_pred_up)\n",
    "\n",
    "# arranging final results\n",
    "final_result =cbind(test_dataset$loan_application_id, w_pred_up)\n",
    "final_resultt = cbind(test_dataset$loan_application_id, as.numeric(w_pred_up)-1)\n",
    "colnames(final_resultt) = c(\"loan_application_id\", \"prediction\")\n",
    "# saving results as csv file\n",
    "write.csv(final_resultt, file=\"final results for test data set\", row.names = FALSE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
